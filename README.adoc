= Apache Spark job server
:hardbreaks:

The goal of this project is to initialize Apache `SparkContext` locally for every unique Spark app/job.
Jobs are executed in the context of so called `container` (own class path loader). By doing this it is possible to execute
jobs in already instantiated `SparkContext`, so much faster. Changed job parameters do not need new `SparkContext` instance.


== Prerequisites

=== http://hadoop.apache.org/docs/current/[Hadoop/HDFS]

The solution works with **Hadoop** version _2.7.1_. Hadoop is used mainly as a distributed file system - HDFS.
The installation instructions for pseudo-distributed mode: http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html#Pseudo-Distributed_Operation[Hadoop Installation].
Great explanation of the OSX installation procesess: http://joeyoung.io/installing-hadoop-and-yarn-on-os-x-trials-troubleshooting-and-work-arounds/[OSX Installation].

Once Haoop is installed and configured the following instructions start the name and data nodes.

----
    $ cd /usr/local/hadoop
    $ ./sbin/start-dfs.sh
----    
    
=== http://spark.apache.org[Apache Spark] - standalone version

Apache Spark is a cluster computing system. We tend to use http://spark.apache.org/docs/latest/spark-standalone.html[standalone] installation.

Installation hints: go to the http://spark.apache.org/downloads.html[downloads page] and choose:

* Spark release - 1.5.0
* Package type - pre-build with user-provided Hadoop (we assume that hadoop is already installed).

Unpack the distribution and open `conf/spark-env.sh` file. 
Add the following settings.

----
    # MANDATORY: the explicit path to 'hadoop' binary
    export SPARK_DIST_CLASSPATH=$(/usr/local/hadoop/bin/hadoop classpath)

    # OPTIONAL - SPARK_MASTER_IP, to bind the master to a different IP address or hostname
    SPARK_MASTER_IP=localhost
----

==== Apache Spark recovery settings

Apache Spark has built-in support for slave/worker nodes failures. But the master node is single point of failure.
The description of high availability configuration with ZooKeeper can be find in http://spark.apache.org/docs/latest/spark-standalone.html#high-availability[documentation].

If we want to keep single node configuration between master node restarts we can setup the following property in `conf/spark-env.sh`.

----
    # set config properties for all daemons (e.g. "-Dx=y")
    SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=FILESYSTEM -Dspark.deploy.recoveryDirectory=recovery"
----

In case of configuration with `ZooKeeper` recovery settings should be in `ZooKeeper`. 

=== Import example access log data into the HDFS

We assume that the current user is hadoop's user too. So create folder structure in hdfs directly. 

----
    $ echo $USER
    $ hdfs dfs -mkdir -p /user/$USER/input
----

Import example access log data into the hdfs file system.

----
    $ cd spark-module-topn/src/test/resources/input
    $ hdfs dfs -copyFromLocal access*.log /user/$USER/input
----

=== Import example movie recommendation data into the HDFS

We assume that the current user is hadoop's user too. So create folder structure in hdfs directly. 

----
    $ echo $USER
    $ hdfs dfs -mkdir -p /user/$USER/input_movie
----

Import example movie recommendation data into the hdfs file system.

----
    $ cd spark-module-movie-recommendation/src/test/resources/ml-latest-small
    $ hdfs dfs -copyFromLocal *.csv /user/$USER/input_movie
----

== Settings


=== ratpack-app/src/ratpack/application.properties

IMPORTANT: you can also override the settings by putting `config/application.properties` file on the class path. 
For example by placing it in `ratpack-app/src/ratpack/config/application.properties`.

spark.libsDir:: a path to folder with all Apache Spark dependencies. 
It is the results of `./gradlew :spark-module-deps:installDist` command.
----
  spark.libsDir=/Users/zedar/dev/hadoopdev/ratpack-hadoop-spark/spark-module-deps/build/install/lib/
----

spark.homeDir:: a path to folder where apache spark is installed. This is not used in server mode.
----
    spark.homeDir=/Users/zedar/dev/hadoopdev/spark-1.5.0-bin-without-hadoop
----

spark.extraJavaOptions:: an additional parameters for job execution
----
    spark.extraJavaOptions=
----

spark.user:: a user in context of which we access spark server
----
    spark.user=zedar
----

spark.master:: an address of the Spark server either local, or standalone.
----
    spark.master=spark://localhost:7077
----

If Spark is configured in High Availability, all the master nodes should be put in one line
----
    spark.master=spark://master.server1:9077,master.server2:9077
----

spark.maxCoresPerTask:: a maximum number of cores to be used by one Spark job. Default value is `2`.
----
    spark.maxCoresPerTask=2
----

spark.fileSystemHost:: a HDFS file system server
----
    spark.fileSystemHost=localhost
----

spark.fileSystemPort:: a HDFS file system port
----
    spark.fileSystemPort=54310
----

=== ratpack-app/src/ratpack/sparkjobs.properties

IMPORTANT: you can also override the settings by putting `config/sparkjobs.properties` file on the class path. For example by
placing it in `ratpack-app/src/ratpack/config/sparkjobs.properties`.

job.jarPaths[n]:: a collection of paths with jars that should be put on the SparkContext's classpath
----
    job.jarPaths[0]=/Users/zedar/dev/hadoopdev/ratpack-spark-job-server/spark-module-topn/build/libs/
    job.jarPaths[1]=/Users/zedar/dev/hadoopdev/ratpack-spark-job-server/spark-module-movie-recommendation/build/libs/
----

job.classNames[n]:: all the jars that contain the class names should be transfered with SparkContext
----
    job.classNames[0]=spark.jobserver.JobAPI
----

job.jobs:: a definition of jobs code names to their class names. 
The class names indicate the jars that should be transferred with SparkContext.
----
    job.jobs=TOPN=spark.func.topn.TopNApp,MOVIEREC=spark.func.movierecommendation.MovieRecommendationApp
----

== Spark and multiple SparkContexts in the same JVM

The 1.5 Apache Spark version assumes that there is only one instance of the `SparkContext` per JVM. 
Even if it is possible to set property `spark.driver.allowMultipleContexts` to `true`, exceptions are thrown:
----
    org.apache.spark.SparkException: Failed to get broadcast
----
That is the reason why containers share the `SparkContext` between all jobs and job requests.

See the discussion https://issues.apache.org/jira/browse/SPARK-2243[SPARK-2243].

== Spark Job Container

Apache Spark jobs are executed in their own containers, that share one/common instance of the `SparkContext`.
Initialization of `SparkContext` is expensive and has to be one threaded. 
So all requests to the same container have to wait for `SparkContext` initialization.

A container loads all dependecies throughout own class loader. 
Then conflicts between Ratpack and Spark `jars` are then eliminated.

== Build and run the job server

Module: *spark-module-deps*:: all dependencies necessary to execute Spark jobs. They are not included directly in 
Ratpack application because of so many conflicts. 
They are loaded in seperate class loader used by so called _containers_ holding _SparkContext_.

Module: *spark-module-topn*:: implementation of calculating the most active N users based on the specific access log.

Module: *spark-module-movie-recommendation*:: implementation of movie recommendation algorithm. 
Algorithm uses Spark MLib (Machine Learning Library) and finds the best movies for the given user.

Module: *ratpack-app*:: the server with endpoints to execute Spark jobs

Every Spark job is executed in separate `Container`. There is a hirarchy of class loaders. The common (root) class loader 
with all Apache Spark dependencies. Every `Container` has its own class loader with common (root) set as parent.

Starting the server with building all dependencies:
----
    $ ./gradlew run
----
Starting and executing the **TopN** job:
----
    $ curl -XPOST -H "Content-Type: application/json" -d '{"mode": "SYNC", "codeName": "TOPN", "params": [{"name": "inputDir", "value": "/user/zedar/input"}, {"name": "outputDir", "value": "/user/zedar/output"}, {"name": "limit", "value": 5}, {"name": "dateFrom", "value": "2015-07-13"}, {"name": "dateTo", "value": "2015-11-30"}]}' http://localhost:5050/v1/spark/jobs
----
Note, that the first execution of the job could take more time and, it is very important, blocks the other executions of the same job.
This is, because initialization of Spark Job on the Spark server, deployment of the application in the claster takes some time.
The next job executions should be much faster.

Starting and executing the *MovieRecommendation* job:
----
    $ curl -XPOST -H "Content-Type: application/json" -d '{"mode": "SYNC", "codeName": "MOVIEREC", "params": [{"name": "inputDir", "value": "/user/zedar/input_movie"}, {"name": "outputDir", "value": "/user/zedar/output_movie"}, {"name": "userId", "value": 12}, {"name": "limit", "value": 15}]}' http://localhost:5050/v1/spark/jobs
----
Starting and executing any of the registered jobs:
----
    $ curl -v -XPOST -H "Content-Type: application/json" -d '{"mode": "SYNC", "codeName": "TOPN", "params":[{"name": "name1", "value": "value1"}]}' http://localhost:5050/v1/spark/jobs
----
== Job Server API

include::ratpack-app/docs/apidef/index.adoc[]

== Spark module/job interface

Spark modules are gradle subprojects. They are java libraries (jars) with dependencies to `spark-core` and `spark-mlib`.
Spark modules have to provide a class implementing `spark.jobserver.JobApi` interface.